# Supabase Configuration
# Development Environment
DEVELOPMENT_SUPABASE_URL=                # Your local/development Supabase URL
DEVELOPMENT_SUPABASE_KEY=                # IMPORTANT: Must be the SERVICE ROLE KEY, not anon key

# Production Environment
PRODUCTION_SUPABASE_URL=                 # Your production Supabase URL
PRODUCTION_SUPABASE_KEY=                 # IMPORTANT: Must be the SERVICE ROLE KEY, not anon key

# Note: We use service role keys because:
# 1. The AI chat needs to read any user's messages
# 2. We need to bypass Row Level Security (RLS)
# 3. This is a backend service, not a frontend application

# Environment
NODE_ENV=                     # development or production
PORT=

# Build Configuration
NEXT_DISABLE_ESLINT=
NEXT_DISABLE_TYPE_CHECKS=

# Pinecone Configuration
PINECONE_API_KEY=                        # Your Pinecone API key

# Development Indexes
DEVELOPMENT_PINECONE_LARGE_INDEX=        # Development index for large embeddings
DEVELOPMENT_PINECONE_SMALL_INDEX=        # Development index for small embeddings

# Production Indexes
PRODUCTION_PINECONE_LARGE_INDEX=         # Production index for large embeddings
PRODUCTION_PINECONE_SMALL_INDEX=         # Production index for small embeddings

# OpenAI Configuration
OPENAI_API_KEY=

# API Security
TENSAI_KEY=                   # Required for accessing the chat endpoint

# Vercel Configuration
CRON_SECRET=

# Development vs Production Notes:
# 1. Use different Supabase projects for dev/prod
# 2. Use different Pinecone indexes for dev/prod
# 3. Can use same OpenAI key for both, but separate keys recommended
# 4. Set NODE_ENV appropriately for each environment
# 5. Use different TENSAI_KEY values for dev/prod environments
# 6. DEVELOPMENT_SUPABASE_KEY should be service role key for local instance
# 7. PRODUCTION_SUPABASE_KEY should be service role key for production instance
# 8. Each environment has its own Pinecone indexes for message embeddings

# Add any additional environment-specific variables below 